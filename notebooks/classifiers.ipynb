{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Topic Classification\n",
    "\n",
    "While some of the research outputs supplied by users could be identified and therefore programmatically classified into the two broad categories of \"Life Sciences\" and \"Earth Sciences\", there were far more that could not be identified. In order to make an estimate of the types of projects being completed, it is necessary to attempt to classify these remaining outputs.\n",
    "\n",
    "This notebook details an attempt to train a Support Vector Machine (SVM) classifier and a Naive Bayes classifier in order to categorise the outputs based on their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlitedict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of journals represented by the identified outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2041-4811\n",
      "1464-0678\n",
      "0030-6053\n"
     ]
    }
   ],
   "source": [
    "metadata = sqlitedict.SqliteDict('../synth/data/doi_metadata.db')\n",
    "journal_list = []\n",
    "for k, v in metadata.items():\n",
    "    issn = v.get('ISSN', [])\n",
    "    journal_list += issn\n",
    "journal_list = list(set(journal_list))\n",
    "\n",
    "for j in journal_list[:3]:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the ASJC data from this page: https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1000', 'Multidisciplinary', 'Multidisciplinary')\n",
      "('1100', 'General Agricultural and Biological Sciences', 'Life Sciences')\n",
      "('1101', 'Agricultural and Biological Sciences (miscellaneous)', 'Life Sciences')\n",
      "('1000', 'Multidisciplinary')\n",
      "('1100', 'Life Sciences')\n",
      "('1101', 'Life Sciences')\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus')\n",
    "page = BeautifulSoup(response.content)\n",
    "table_rows = page.find('table').find_all('tr')[1:]\n",
    "all_asjc = [tuple([cell.text for cell in row.find_all('td')]) for row in table_rows]\n",
    "asjc = {x[0]: x[2] for x in all_asjc}\n",
    "\n",
    "for x in all_asjc[:3]:\n",
    "    print(x)\n",
    "\n",
    "for x in list(asjc.items())[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorise each journal based on the subjects it's tagged with on CrossRef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "177\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from crossref.restful import Etiquette, Journals\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# multithreading speeds the download process up\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "etiquette = Etiquette('SYNTH transform', '0.1', 'https://github.com/NaturalHistoryMuseum/synth_transform',\n",
    "                      'data@nhm.ac.uk')\n",
    "journal_api = Journals(etiquette=etiquette)\n",
    "\n",
    "if os.path.exists('journals.json'):\n",
    "    with open('journals.json', 'r') as f:\n",
    "        all_issns = json.load(f)\n",
    "else:\n",
    "    all_issns = []\n",
    "\n",
    "    def get_journal(issn):\n",
    "        journal = journal_api.journal(issn)\n",
    "        if journal is None:\n",
    "            return\n",
    "        subjects = journal.get('subjects', [])\n",
    "        top_level_subjects = Counter([asjc.get(str(s['ASJC'])) for s in subjects])\n",
    "\n",
    "        # for each category, make sure there's no overlap between subjects\n",
    "        if top_level_subjects['Life Sciences'] > 0 and top_level_subjects['Physical Sciences'] == 0:\n",
    "            all_issns.append((issn, 'life'))\n",
    "        elif top_level_subjects['Physical Sciences'] > 0 and top_level_subjects['Life Sciences'] == 0:\n",
    "            all_issns.append((issn, 'earth'))\n",
    "        elif len(top_level_subjects) > 0 and top_level_subjects['Life Sciences'] == 0 and top_level_subjects['Physical Sciences'] == 0:\n",
    "            all_issns.append((issn, 'other'))\n",
    "\n",
    "    with ThreadPoolExecutor(10) as thread_executor:\n",
    "        thread_map(get_journal, journal_list)\n",
    "        \n",
    "    with open('journals.json', 'w') as f:\n",
    "        json.dump(all_issns, f)\n",
    "        \n",
    "print(len([j for j in all_issns if j[1] == 'life']))\n",
    "print(len([j for j in all_issns if j[1] == 'earth']))\n",
    "print(len([j for j in all_issns if j[1] == 'other']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sample of articles from each journal, attempting to ignore irrelevant results such as front/back matter, tables of contents, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "if os.path.exists('titles.json'):\n",
    "    # load from json file if possible because downloading new results will take quite a while\n",
    "    with open('titles.json', 'r') as f:\n",
    "        work_titles = json.load(f)\n",
    "else:\n",
    "    work_titles = {\n",
    "        'earth': [],\n",
    "        'life': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    ignore = [re.compile(i) for i in \n",
    "              ['(front|back) matter',\n",
    "               'special issue',\n",
    "               'price\\W',\n",
    "               '(volume|issue) \\d']\n",
    "             ]\n",
    "    \n",
    "    def iter_works(issn, add_to):\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                works = list(journal_api.works(issn).sample(100))\n",
    "                break\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                works = []\n",
    "                continue\n",
    "        for work in works:\n",
    "            title = work.get('title')\n",
    "            if title is None or len(title) == 0:\n",
    "                continue\n",
    "            title = title[0].lower()\n",
    "            if len(title.split(' ')) < 5:\n",
    "                # ignore it if it has fewer than 5 words in the title - these are usually not articles\n",
    "                continue\n",
    "            if any([rgx.search(title) is not None for rgx in ignore]):\n",
    "                continue\n",
    "            work_titles[add_to].append(title)\n",
    "        \n",
    "    \n",
    "    with ThreadPoolExecutor(10) as thread_executor:\n",
    "        thread_map(lambda x: iter_works(*x), all_issns)\n",
    "        \n",
    "    with open('titles.json', 'w') as f:\n",
    "        json.dump(work_titles, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the titles into data that can be used to train the classifier by:\n",
    "1. removing punctuation (except hyphens);\n",
    "2. discarding words that aren't nouns or adjectives;\n",
    "3. stemming words so that e.g. \"geology\" and \"geological\" are both counted as the same word;\n",
    "4. discarding the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text  label\n",
      "0               co enrich yield florunn cultivar      0\n",
      "1  long term qualiti stabil assess cryosat- data      0\n",
      "2            magnet boundari outer planet review      0\n",
      "3               factor photocatalyt oxid ethylen      0\n",
      "4  weather disturb low latitud low altitud model      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "no_punct_rgx = re.compile(r'[^a-z- ]')\n",
    "en_em_dash_rgx = re.compile(r'\\s-\\s')\n",
    "\n",
    "\n",
    "if os.path.exists('training_data.csv'):\n",
    "    # again, read from a file if available because this might take a while\n",
    "    df = pd.read_csv('training_data.csv', index_col=0)\n",
    "else:\n",
    "    def process_texts(texts):\n",
    "        token_lists = []\n",
    "\n",
    "        def get_tokens(txt):\n",
    "            txt = no_punct_rgx.sub(' ', txt.lower())\n",
    "            txt = en_em_dash_rgx.sub(' ', txt)\n",
    "            doc = nlp(txt)\n",
    "            tokens = [stemmer.stem(token.text) for token in doc if token.pos_ in ['NOUN', 'ADJ'] and len(token.lemma_) > 1]\n",
    "            token_lists.append(tokens)\n",
    "\n",
    "        with ThreadPoolExecutor(10) as thread_executor:\n",
    "            thread_map(get_tokens, texts)\n",
    "\n",
    "        all_tokens = [t for sublist in token_lists for t in set(sublist)]\n",
    "        most_common = [k for k, v in sorted(Counter(all_tokens).items(), key=lambda x: -x[1])][:20]\n",
    "        print(most_common)\n",
    "\n",
    "        output = [' '.join([token for token in doc if token not in most_common]) for doc in token_lists]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    labels = {'earth': 0,\n",
    "              'life': 1,\n",
    "              'other': 2}\n",
    "\n",
    "    # transform the data into (title, label) tuples\n",
    "    data = [(x, labels[k]) for k, v in work_titles.items() for x in v]\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "    df.text = process_texts(df.text)\n",
    "    df = df.where(df != '')\n",
    "    df = df.dropna(axis=0)\n",
    "    df.to_csv('training_data.csv')\n",
    "    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training group and a testing group, then create a vectoriser to get a numerical representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 777)\t0.39119454696540323\n",
      "  (0, 591)\t0.3626417838975834\n",
      "  (0, 224)\t0.394709539287537\n",
      "  (0, 175)\t0.4012984656187292\n",
      "  (0, 114)\t0.6313649373251458\n",
      "  (1, 994)\t0.5744471348422607\n",
      "  (1, 604)\t0.5156354922310193\n",
      "  (1, 133)\t0.6357126146484683\n",
      "  (2, 959)\t0.3772342053171079\n",
      "  (2, 895)\t0.4348512077415644\n",
      "  (2, 611)\t0.5782015934585747\n",
      "  (2, 597)\t0.4047166439878694\n",
      "  (2, 79)\t0.41289966924863764\n",
      "  (3, 363)\t1.0\n",
      "  (4, 887)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('train_split.csv') and os.path.exists('test_split.csv'):\n",
    "    training_data = pd.read_csv('train_split.csv', index_col=0)\n",
    "    test_data = pd.read_csv('test_split.csv', index_col=0)\n",
    "else:\n",
    "    training_data, test_data = train_test_split(df, test_size=0.2, stratify=df.label)\n",
    "    training_data.to_csv('train_split.csv')\n",
    "    test_data.to_csv('test_split.csv')\n",
    "\n",
    "if os.path.exists('vectoriser.pkl'):\n",
    "    with open('vectoriser.pkl', 'rb') as f:\n",
    "        vectoriser = pickle.load(f)\n",
    "else:\n",
    "    vectoriser = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    vectoriser.fit(training_data.text)\n",
    "    with open('vectoriser.pkl', 'wb') as f:\n",
    "        pickle.dump(vectoriser, f)\n",
    "\n",
    "features = vectoriser.transform(training_data.text)\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use those features to train a Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.01%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.74      3102\n",
      "           1       0.81      0.95      0.87      6755\n",
      "           2       0.62      0.04      0.08       687\n",
      "\n",
      "    accuracy                           0.81     10544\n",
      "   macro avg       0.75      0.56      0.56     10544\n",
      "weighted avg       0.80      0.81      0.78     10544\n",
      "\n",
      "[[2096  996   10]\n",
      " [ 330 6418    7]\n",
      " [ 107  552   28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from datetime import datetime as dt\n",
    "\n",
    "if os.path.exists('svc.model'):\n",
    "    with open('svc.model', 'rb') as f:\n",
    "        svc_classifier = pickle.load(f)\n",
    "else:\n",
    "    svc_classifier = SVC()\n",
    "    print('Fitting SVC...')\n",
    "    start = dt.now()\n",
    "    svc_classifier.fit(features, training_data.label)\n",
    "    print(f'Done ({round((dt.now() - start).total_seconds())}s)')\n",
    "    with open('svc.model', 'wb') as f:\n",
    "        pickle.dump(svc_classifier, f)\n",
    "\n",
    "svc_predicted = svc_classifier.predict(vectoriser.transform(test_data.text))\n",
    "print(f'Accuracy: {round(accuracy_score(test_data.label, svc_predicted) * 100, 2)}%')\n",
    "print(classification_report(test_data.label, svc_predicted))\n",
    "print(confusion_matrix(test_data.label, svc_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it can be used to train a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.92%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.72      3102\n",
      "           1       0.79      0.95      0.86      6755\n",
      "           2       0.58      0.04      0.08       687\n",
      "\n",
      "    accuracy                           0.80     10544\n",
      "   macro avg       0.73      0.54      0.55     10544\n",
      "weighted avg       0.79      0.80      0.77     10544\n",
      "\n",
      "[[2096  996   10]\n",
      " [ 330 6418    7]\n",
      " [ 107  552   28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "if os.path.exists('nb.model'):\n",
    "    with open('nb.model', 'rb') as f:\n",
    "        nb_classifier = pickle.load(f)\n",
    "else:\n",
    "    nb_classifier = MultinomialNB()\n",
    "    print('Fitting Naive Bayes...')\n",
    "    start = dt.now()\n",
    "    nb_classifier.fit(features, training_data.label)\n",
    "    print(f'Done ({round((dt.now() - start).total_seconds())}s)')\n",
    "    with open('nb.model', 'wb') as f:\n",
    "        pickle.dump(nb_classifier, f)\n",
    "\n",
    "nb_predicted = nb_classifier.predict(vectoriser.transform(test_data.text))\n",
    "print(f'Accuracy: {round(accuracy_score(test_data.label, nb_predicted) * 100, 2)}%')\n",
    "print(classification_report(test_data.label, nb_predicted))\n",
    "print(confusion_matrix(test_data.label, svc_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the best-performing classifier (the SVM classifier) to estimate the broad category of titles in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular phylogeny within true bugs (Hemiptera: Miridae).\n",
      "Gene-flow solid frozen - the roles of intrinsic and extrinsic factors on microevolution of Antarctic shelf fishes\n",
      "Age and rate of speciation in the adaptive radiation of antarctic fishes (Trematominae)\n",
      "Did glacial advances during the Pleistocene influence differently the demographic histories of benthic and pelagic Antarctic shelf fishes? – Inferences from intraspecific mitochondrial and nuclear DNA sequence diversity\n",
      "Contribution to the Pupae of the Western Palearctic Tiger Moths (Lepidoptera, Noctuoidea, Arctiidae).\n"
     ]
    }
   ],
   "source": [
    "from synth.model.analysis import Output\n",
    "from synth.utils import Config, Context\n",
    "import yaml\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "with open('../config.yml', 'r') as f:\n",
    "    config = Config(**yaml.safe_load(f))\n",
    "\n",
    "context = Context(config)\n",
    "session = sessionmaker(bind=context.target_engine)()\n",
    "\n",
    "titles = [t[0] for t in session.query(Output.title).filter(Output.title.isnot(None)).all()]\n",
    "\n",
    "for t in titles[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86cbd2c1d6448088affe20f1fbe83ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9594.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "true bug\n",
      "contribut pupa western palearct moth noctuoidea\n",
      "\n",
      "gene flow solid role intrins extrins factor microevolut antarct shelf fish\n",
      "age rate speciat adapt radiat antarct fish\n"
     ]
    }
   ],
   "source": [
    "# use the same preprocessing as earlier, except without discarding common words\n",
    "\n",
    "processed_titles = []\n",
    "\n",
    "def get_tokens(txt):\n",
    "    txt = no_punct_rgx.sub(' ', txt.lower())\n",
    "    txt = en_em_dash_rgx.sub(' ', txt)\n",
    "    doc = nlp(txt)\n",
    "    tokens = [stemmer.stem(token.text) for token in doc if token.pos_ in ['NOUN', 'ADJ'] and len(token.lemma_) > 1]\n",
    "    processed_titles.append(' '.join(tokens))\n",
    "\n",
    "with ThreadPoolExecutor(10) as thread_executor:\n",
    "    thread_map(get_tokens, titles)\n",
    "    \n",
    "for t in processed_titles[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life     7722\n",
      "earth    1848\n",
      "other      24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "transformed_titles = vectoriser.transform(processed_titles)\n",
    "predictions = pd.Series(svc_classifier.predict(transformed_titles))\n",
    "\n",
    "predictions = predictions.replace(0, 'earth').replace(1, 'life').replace(2, 'other')\n",
    "\n",
    "print(predictions.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARTH:\n",
      "A new spurless Habenella species (Orchidaceae) from Nepal\n",
      "The relationships within Siphini Mordvilko, 1928 (Hemiptera, Aphidoidea:Chaitophorinae)\n",
      "Dorcatherium naui and pecoran ruminants from the late Middle Miocene Gratkorn locality (Austria)\n",
      "Cyclocystoids (Echinodermata: Echinozoa) from the Ordovician of Sweden\n",
      "Structure and composition of the nacre–prisms transition in the shell of Pinctada margaritifera (Mollusca, Bivalvia)\n",
      "The Popocatépetl Volcanic Complex: new geological map and constraints on its complex growth through periodic edifice construction and destruction\n",
      "Early Triassic Saurichthys from Greenland and Madagascar\n",
      "Did the Mediterranean marine reflooding precede the Mio–Pliocene boundary? Paleontological and geochemical evidence from upper Messinian sequences of Tuscany, Italy\n",
      "Flora of Ecuador: Euphorbiaceae tribe Hippomaneae\n",
      "Chitons (Mollusca: Polyplacophora) from the Middle Miocene sandy facies of Ukraine, Central Paratethys\n",
      "\n",
      "\n",
      "LIFE:\n",
      "Revision of the three-striped species of Phyllogomphus (Odonata, Gomphidae)\n",
      "Shallow-water bryozoans from the Azores (central North Atlantic): native vs. non-indigenous species, and a method to evaluate taxonomic uncertainty\n",
      "Pierolapithecus and the functional morphology of Miocene ape hand phalanges: paleobiological and evolutionary implications\n",
      "Taxonomic revision within the subtribe Odontocheilina in a new sense 10. Odontocheila castelnaui species-group (Coleoptera: Cicindelidae).\n",
      "Is the Lower tectonic unit (Lower terain) in the Rhodope a monolithic tectonic unit?\n",
      "Echiniscus ganczareki, a new species of Tardigrada (Heterotardigrada: Echiniscidae, bigranulatus group) from Costa Rica\n",
      "Flower development of Meliosma (Sabiaceae): evidence for multiple origins of pentamery in the eudicots\n",
      "Ancient diversity of Afrotropical Microborus: three endemic species – not one widespread\n",
      "Taxonomic reassessment of all passerines in Western Palearctic\n",
      "A New Genus and Species of Oriental Colobathristidae (Hemiptera: Heteroptera) with a Key to Eastern Hemisphere Genera and Morphological and Functional Considerations\n",
      "\n",
      "\n",
      "OTHER:\n",
      "Phylogenetic Relationships of Species of Raymunida (Decapoda: Galatheidae) Based on Morphology and Mitochondrial Cytochrome Oxidase Sequences, with the Recognition of Four New Species\n",
      "The use of ecological modelling to define priority areas for poorly surveyed taxa: bryophytes at country scale as examples\n",
      "Evolution of floral characters in the genus Gunnera\n",
      "«Πηγές του Αγγίτη στη λεκάνη της Δράμας. Τα κεραμικά σύνολα από το εσωτερικό του σπηλαίου», ΑΑΑ 39 (2006), Σύμμεικτα, 107-138.\n",
      "«Προμαχών-Τοpolnica. Tα βούκρανα του μεγάλου υπόσκαφου χώρου: ζωολογικός προσδιορισμός και πολιτισμικά παράλληλα από την ανατολική Μεσόγειο», ΑΕΜΘ 20, 2006, 217-228.\n",
      "Catalogue of the pseudoscorpion types (Arachnida: Pseudoscorpiones) in the Museum für Naturkunde, Berlin\n",
      "Companions from the oldest Times: Dogs in Ancient Greek Literature, Iconography and Osteological Testimony\n",
      "From Mesolithic Fishermen and Bird Hunters to Neolithic Goat Herders:\n",
      "The osteology and phylogenetic relationships of the Middle Jurassic sauropod Atlasaurus imelakei from the High Atlas Mountains in Morocco.\n",
      "Reconstruction of the environmental evolution of a Sicilian saltmarsh (Italy)\n"
     ]
    }
   ],
   "source": [
    "classified_titles = pd.DataFrame({'text': pd.Series(titles), 'label': predictions})\n",
    "\n",
    "print('EARTH:')\n",
    "for title in classified_titles[classified_titles.label=='earth'].sample(10).text:\n",
    "    print(title)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('LIFE:')\n",
    "for title in classified_titles[classified_titles.label=='life'].sample(10).text:\n",
    "    print(title)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('OTHER:')\n",
    "for title in classified_titles[classified_titles.label=='other'].sample(10).text:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so few \"other\" items, the classifier cannot accurately identify these. We can attempt to train a new classifier ignoring all \"other\" items, making the assumption that all items fall into either \"earth\" or \"life\" sciences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVC...\n",
      "Done (88s)\n",
      "Accuracy: 86.37%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.67      0.76      3102\n",
      "         1.0       0.86      0.95      0.91      6755\n",
      "\n",
      "    accuracy                           0.86      9857\n",
      "   macro avg       0.86      0.81      0.83      9857\n",
      "weighted avg       0.86      0.86      0.86      9857\n",
      "\n",
      "[[2088 1014]\n",
      " [ 330 6425]]\n",
      "\n",
      "\n",
      "life     7738\n",
      "earth    1856\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "EARTH:\n",
      "Zircon and muscovite ages, geochemistry, and Nd–Hf isotopes for the Aktyuz metamorphic terrane: Evidence for an Early Ordovician collisional belt in the northern Tianshan of Kyrgyzstan\n",
      "A NEW VESPERTILIONID BAT FROM THE ORIENTAL REGION\n",
      "Detrital zircon geochronology of Jurassic sandstones of western Cuba (San Cayetano Formation): Implications for the Jurassic paleogeography of the NW Proto-Caribbean\n",
      "Palaeozoic evolution of the Eastern Cordillera of Peru\n",
      "The bird remains from the West Runton Freshwater Bed, Norfolk, England\n",
      "Differentiation of sex chromosomes and karyotypic evolution in the eye-lid geckos (Squamata: Gekkota: Eublepharidae), a group with different modes of sex determination\n",
      "Distribution of sulphated polysaccharides within calcareous biominerals suggests a widely shared two-step crystallization process for the microstructural growth units\n",
      "Estimating the onset of dispersal in endangered Bonelli's Eagles Hieraaetus fasciatus tracked by satellite telemetry: a comparison between methods\n",
      "Large mammal biochronology framework in Europe at Jaramillo: The Epivillafranchian as a formal biochron\n",
      "Stephanorhinus hundsheimensis (Mammalia, Rhinocerotidae) from the late early Pleistocene deposits of the Denizli Basin (Anatolia, Turkey)\n",
      "\n",
      "\n",
      "LIFE:\n",
      "Potentially toxic species of Pseudo-nitzschia in Northern Adriatic Sea, recent observations\n",
      "Description of a new species belonging to the Murina “suilla-group” (Chiroptera: Vespertilionidae: Murininae) from north Vietnam.\n",
      "Annotated list of Ensifera (Orthoptera) and further records on Caelifera (Orthoptera) of Mt Kilimanjaro, Tanzania\n",
      "Teeth and jaws: getting closer to inferring feeding behaviour from morphometric traits in hominins.\n",
      "Notizen zur Heuschreckenfauna (Insecta, Orthoptera) von Fuerteventura (Kanarische Inseln, Spanien).\n",
      "Taxonomic and nomenclatural review of the Onthophagus of the group 27 sensu d'Orbigny (1913), and notes on Onthophagus columbianus Boucomont, 1932 (Coleoptera: Scarabaeidae: Scarabaeinae).\n",
      "Intraspecific variation in clutch size and maternal investment in pueriparous and larviparous Salamandra salamandra females\n",
      "Metagranites from Berkovska Group, Central Balkan area – structural and petrographical evidences\n",
      "Segmentation of labium in Heteroptera\n",
      "The systematic position and phylogenetic relationships of Asiobaccha Violovitsh (Diptera, Syrphidae)\n"
     ]
    }
   ],
   "source": [
    "df_2 = df.where(df.label != 2).dropna(axis=0)\n",
    "\n",
    "training_data_2, test_data_2 = train_test_split(df_2, test_size=0.2, stratify=df_2.label)\n",
    "\n",
    "vectoriser_2 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "features_2 = vectoriser_2.fit_transform(training_data_2.text)\n",
    "\n",
    "svc_classifier_2 = SVC()\n",
    "print('Fitting SVC...')\n",
    "start = dt.now()\n",
    "svc_classifier_2.fit(features_2, training_data_2.label)\n",
    "print(f'Done ({round((dt.now() - start).total_seconds())}s)')\n",
    "\n",
    "svc_predicted_2 = svc_classifier_2.predict(vectoriser_2.transform(test_data_2.text))\n",
    "print(f'Accuracy: {round(accuracy_score(test_data_2.label, svc_predicted_2) * 100, 2)}%')\n",
    "print(classification_report(test_data_2.label, svc_predicted_2))\n",
    "print(confusion_matrix(test_data_2.label, svc_predicted_2))\n",
    "\n",
    "transformed_titles_2 = vectoriser_2.transform(processed_titles)\n",
    "predictions_2 = pd.Series(svc_classifier_2.predict(transformed_titles_2))\n",
    "\n",
    "predictions_2 = predictions_2.replace(0, 'earth').replace(1, 'life')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(predictions_2.value_counts())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "classified_titles_2 = pd.DataFrame({'text': pd.Series(titles), 'label': predictions_2})\n",
    "\n",
    "print('EARTH:')\n",
    "for title in classified_titles_2[classified_titles_2.label=='earth'].sample(10).text:\n",
    "    print(title)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('LIFE:')\n",
    "for title in classified_titles_2[classified_titles_2.label=='life'].sample(10).text:\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
