{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Topic Classification\n",
    "\n",
    "While some of the research outputs supplied by users could be identified and therefore programmatically classified into the two broad categories of \"Life Sciences\" and \"Earth Sciences\", there were far more that could not be identified. In order to make an estimate of the types of projects being completed, it is necessary to attempt to classify these remaining outputs.\n",
    "\n",
    "This notebook details an attempt to train a Support Vector Machine (SVM) classifier and a Naive Bayes classifier in order to categorise the outputs based on their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlitedict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of journals represented by the identified outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572-9699\n",
      "2032-3913\n",
      "1026-2296\n"
     ]
    }
   ],
   "source": [
    "metadata = sqlitedict.SqliteDict('../synth/data/doi_metadata.db')\n",
    "journal_list = []\n",
    "for k, v in metadata.items():\n",
    "    issn = v.get('ISSN', [])\n",
    "    journal_list += issn\n",
    "journal_list = list(set(journal_list))\n",
    "\n",
    "for j in journal_list[:3]:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the ASJC data from this page: https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1000', 'Multidisciplinary', 'Multidisciplinary')\n",
      "('1100', 'General Agricultural and Biological Sciences', 'Life Sciences')\n",
      "('1101', 'Agricultural and Biological Sciences (miscellaneous)', 'Life Sciences')\n",
      "('1000', 'Multidisciplinary')\n",
      "('1100', 'Life Sciences')\n",
      "('1101', 'Life Sciences')\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus')\n",
    "page = BeautifulSoup(response.content)\n",
    "table_rows = page.find('table').find_all('tr')[1:]\n",
    "all_asjc = [tuple([cell.text for cell in row.find_all('td')]) for row in table_rows]\n",
    "asjc = {x[0]: x[2] for x in all_asjc}\n",
    "\n",
    "for x in all_asjc[:3]:\n",
    "    print(x)\n",
    "\n",
    "for x in list(asjc.items())[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorise each journal based on the subjects it's tagged with on CrossRef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "177\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from crossref.restful import Etiquette, Journals\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# multithreading speeds the download process up\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "etiquette = Etiquette('SYNTH transform', '0.1', 'https://github.com/NaturalHistoryMuseum/synth_transform',\n",
    "                      'data@nhm.ac.uk')\n",
    "journal_api = Journals(etiquette=etiquette)\n",
    "\n",
    "if os.path.exists('journals.json'):\n",
    "    with open('journals.json', 'r') as f:\n",
    "        all_issns = json.load(f)\n",
    "else:\n",
    "    all_issns = []\n",
    "\n",
    "    def get_journal(issn):\n",
    "        journal = journal_api.journal(issn)\n",
    "        if journal is None:\n",
    "            return\n",
    "        subjects = journal.get('subjects', [])\n",
    "        top_level_subjects = Counter([asjc.get(str(s['ASJC'])) for s in subjects])\n",
    "\n",
    "        # for each category, make sure there's no overlap between subjects\n",
    "        if top_level_subjects['Life Sciences'] > 0 and top_level_subjects['Physical Sciences'] == 0:\n",
    "            all_issns.append((issn, 'life'))\n",
    "        elif top_level_subjects['Physical Sciences'] > 0 and top_level_subjects['Life Sciences'] == 0:\n",
    "            all_issns.append((issn, 'earth'))\n",
    "        elif len(top_level_subjects) > 0 and top_level_subjects['Life Sciences'] == 0 and top_level_subjects['Physical Sciences'] == 0:\n",
    "            all_issns.append((issn, 'other'))\n",
    "\n",
    "    with ThreadPoolExecutor(10) as thread_executor:\n",
    "        thread_map(get_journal, journal_list)\n",
    "        \n",
    "    with open('journals.json', 'w') as f:\n",
    "        json.dump(all_issns, f)\n",
    "        \n",
    "print(len([j for j in all_issns if j[1] == 'life']))\n",
    "print(len([j for j in all_issns if j[1] == 'earth']))\n",
    "print(len([j for j in all_issns if j[1] == 'other']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sample of articles from each journal, attempting to ignore irrelevant results such as front/back matter, tables of contents, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "if os.path.exists('titles.json'):\n",
    "    # load from json file if possible because downloading new results will take quite a while\n",
    "    with open('titles.json', 'r') as f:\n",
    "        work_titles = json.load(f)\n",
    "else:\n",
    "    work_titles = {\n",
    "        'earth': [],\n",
    "        'life': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    ignore = [re.compile(i) for i in \n",
    "              ['(front|back) matter',\n",
    "               'special issue',\n",
    "               'price\\W',\n",
    "               '(volume|issue) \\d']\n",
    "             ]\n",
    "    \n",
    "    def iter_works(issn, add_to):\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                works = list(journal_api.works(issn).sample(100))\n",
    "                break\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                works = []\n",
    "                continue\n",
    "        for work in works:\n",
    "            title = work.get('title')\n",
    "            if title is None or len(title) == 0:\n",
    "                continue\n",
    "            title = title[0].lower()\n",
    "            if len(title.split(' ')) < 5:\n",
    "                # ignore it if it has fewer than 5 words in the title - these are usually not articles\n",
    "                continue\n",
    "            if any([rgx.search(title) is not None for rgx in ignore]):\n",
    "                continue\n",
    "            work_titles[add_to].append(title)\n",
    "        \n",
    "    \n",
    "    with ThreadPoolExecutor(10) as thread_executor:\n",
    "        thread_map(lambda x: iter_works(*x), all_issns)\n",
    "        \n",
    "    with open('titles.json', 'w') as f:\n",
    "        json.dump(work_titles, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the titles into data that can be used to train the classifier by:\n",
    "1. removing punctuation (except hyphens);\n",
    "2. discarding words that aren't nouns or adjectives;\n",
    "3. stemming words so that e.g. \"geology\" and \"geological\" are both counted as the same word;\n",
    "4. discarding the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            text  label\n",
      "0               co enrich yield florunn cultivar      0\n",
      "1  long term qualiti stabil assess cryosat- data      0\n",
      "2            magnet boundari outer planet review      0\n",
      "3               factor photocatalyt oxid ethylen      0\n",
      "4  weather disturb low latitud low altitud model      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "no_punct_rgx = re.compile(r'[^a-z- ]')\n",
    "en_em_dash_rgx = re.compile(r'\\s-\\s')\n",
    "\n",
    "\n",
    "if os.path.exists('training_data.csv'):\n",
    "    # again, read from a file if available because this might take a while\n",
    "    df = pd.read_csv('training_data.csv', index_col=0)\n",
    "else:\n",
    "    def process_texts(texts):\n",
    "        token_lists = []\n",
    "\n",
    "        def get_tokens(txt):\n",
    "            txt = no_punct_rgx.sub(' ', txt.lower())\n",
    "            txt = en_em_dash_rgx.sub(' ', txt)\n",
    "            doc = nlp(txt)\n",
    "            tokens = [stemmer.stem(token.text) for token in doc if token.pos_ in ['NOUN', 'ADJ'] and len(token.lemma_) > 1]\n",
    "            token_lists.append(tokens)\n",
    "\n",
    "        with ThreadPoolExecutor(10) as thread_executor:\n",
    "            thread_map(get_tokens, texts)\n",
    "\n",
    "        all_tokens = [t for sublist in token_lists for t in set(sublist)]\n",
    "        most_common = [k for k, v in sorted(Counter(all_tokens).items(), key=lambda x: -x[1])][:20]\n",
    "        print(most_common)\n",
    "\n",
    "        output = [' '.join([token for token in doc if token not in most_common]) for doc in token_lists]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    labels = {'earth': 0,\n",
    "              'life': 1,\n",
    "              'other': 2}\n",
    "\n",
    "    # transform the data into (title, label) tuples\n",
    "    data = [(x, labels[k]) for k, v in work_titles.items() for x in v]\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "    df.text = process_texts(df.text)\n",
    "    df = df.where(df != '')\n",
    "    df = df.dropna(axis=0)\n",
    "    df.to_csv('training_data.csv')\n",
    "    \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training group and a testing group, then create a vectoriser to get a numerical representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 777)\t0.39119454696540323\n",
      "  (0, 591)\t0.3626417838975834\n",
      "  (0, 114)\t0.6313649373251458\n",
      "  (0, 175)\t0.4012984656187292\n",
      "  (0, 224)\t0.394709539287537\n",
      "  (1, 604)\t0.5156354922310193\n",
      "  (1, 133)\t0.6357126146484683\n",
      "  (1, 994)\t0.5744471348422607\n",
      "  (2, 895)\t0.4348512077415644\n",
      "  (2, 79)\t0.41289966924863764\n",
      "  (2, 597)\t0.4047166439878694\n",
      "  (2, 959)\t0.3772342053171079\n",
      "  (2, 611)\t0.5782015934585747\n",
      "  (3, 363)\t1.0\n",
      "  (4, 887)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "training_data, test_data = train_test_split(df, test_size=0.2, stratify=df.label)\n",
    "\n",
    "vectoriser = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "features = vectoriser.fit_transform(training_data.text)\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use those features to train a Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVC...\n",
      "Done (164.440875)\n",
      "81.01289833080425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.74      3102\n",
      "           1       0.81      0.95      0.87      6755\n",
      "           2       0.62      0.04      0.08       687\n",
      "\n",
      "    accuracy                           0.81     10544\n",
      "   macro avg       0.75      0.56      0.56     10544\n",
      "weighted avg       0.80      0.81      0.78     10544\n",
      "\n",
      "[[2096  996   10]\n",
      " [ 330 6418    7]\n",
      " [ 107  552   28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from datetime import datetime as dt\n",
    "\n",
    "classifier = SVC()\n",
    "print('Fitting SVC...')\n",
    "start = dt.now()\n",
    "classifier.fit(features, training_data.label)\n",
    "print(f'Done ({(dt.now() - start).total_seconds()})')\n",
    "\n",
    "predicted = classifier.predict(vectoriser.transform(test_data.text))\n",
    "print(accuracy_score(test_data.label, predicted) * 100)\n",
    "print(classification_report(test_data.label, predicted))\n",
    "print(confusion_matrix(test_data.label, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it can be used to train a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Naive Bayes...\n",
      "Done (0.009734)\n",
      "79.92223065250379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.72      3102\n",
      "           1       0.79      0.95      0.86      6755\n",
      "           2       0.58      0.04      0.08       687\n",
      "\n",
      "    accuracy                           0.80     10544\n",
      "   macro avg       0.73      0.54      0.55     10544\n",
      "weighted avg       0.79      0.80      0.77     10544\n",
      "\n",
      "[[2096  996   10]\n",
      " [ 330 6418    7]\n",
      " [ 107  552   28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "print('Fitting Naive Bayes...')\n",
    "start = dt.now()\n",
    "nb_classifier.fit(features, training_data.label)\n",
    "print(f'Done ({(dt.now() - start).total_seconds()})')\n",
    "\n",
    "nb_predicted = nb_classifier.predict(vectoriser.transform(test_data.text))\n",
    "print(accuracy_score(test_data.label, nb_predicted) * 100)\n",
    "print(classification_report(test_data.label, nb_predicted))\n",
    "print(confusion_matrix(test_data.label, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the best-performing classifier (the SVM classifier) to estimate the broad category of titles in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular phylogeny within true bugs (Hemiptera: Miridae).\n",
      "Gene-flow solid frozen - the roles of intrinsic and extrinsic factors on microevolution of Antarctic shelf fishes\n",
      "Age and rate of speciation in the adaptive radiation of antarctic fishes (Trematominae)\n",
      "Did glacial advances during the Pleistocene influence differently the demographic histories of benthic and pelagic Antarctic shelf fishes? – Inferences from intraspecific mitochondrial and nuclear DNA sequence diversity\n",
      "Contribution to the Pupae of the Western Palearctic Tiger Moths (Lepidoptera, Noctuoidea, Arctiidae).\n"
     ]
    }
   ],
   "source": [
    "from synth.model.analysis import Output\n",
    "from synth.utils import Config, Context\n",
    "import yaml\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "with open('../config.yml', 'r') as f:\n",
    "    config = Config(**yaml.safe_load(f))\n",
    "\n",
    "context = Context(config)\n",
    "session = sessionmaker(bind=context.target_engine)()\n",
    "\n",
    "titles = [t[0] for t in session.query(Output.title).filter(Output.title.isnot(None)).all()]\n",
    "\n",
    "for t in titles[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a332d7b450de4edb83ae15abfb439b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9594.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "true bug\n",
      "contribut pupa western palearct moth noctuoidea\n",
      "contribut descript pupa western noctuida\n",
      "gene flow solid role intrins extrins factor microevolut antarct shelf fish\n",
      "compar biogeograph histori old world forest bird phylogeni molecular date\n"
     ]
    }
   ],
   "source": [
    "# use the same preprocessing as earlier, except without discarding common words\n",
    "\n",
    "processed_titles = []\n",
    "\n",
    "def get_tokens(txt):\n",
    "    txt = no_punct_rgx.sub(' ', txt.lower())\n",
    "    txt = en_em_dash_rgx.sub(' ', txt)\n",
    "    doc = nlp(txt)\n",
    "    tokens = [stemmer.stem(token.text) for token in doc if token.pos_ in ['NOUN', 'ADJ'] and len(token.lemma_) > 1]\n",
    "    processed_titles.append(' '.join(tokens))\n",
    "\n",
    "with ThreadPoolExecutor(10) as thread_executor:\n",
    "    thread_map(get_tokens, titles)\n",
    "    \n",
    "for t in processed_titles[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life     7722\n",
      "earth    1848\n",
      "other      24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "transformed_titles = vectoriser.transform(processed_titles)\n",
    "predictions = pd.Series(classifier.predict(transformed_titles))\n",
    "\n",
    "predictions = predictions.replace(0, 'earth').replace(1, 'life').replace(2, 'other')\n",
    "\n",
    "print(predictions.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular and morphological analysis of the Sagitta setosa complex with description of a new species\n",
      "The Types of Anthomyiidae (Diptera) in the Museum für Naturkunde Berlin, Germany\n",
      "Genetic and age relationship of base metal mineralization along the Periadriatic-Balaton Lineament system on the basis of radiogenic isotope studies\n",
      "ENTHESOPATHIES AND PREHISTORIC HUMAN ACTIVITIES - Methodological approach and application to European Upper Palaeolithic and Mesolithic human fossils\n",
      "The Portalón at Cueva Mayor (Sierra de Atapuerca, Spain): a new archaeological sequence.\n",
      "\n",
      "\n",
      "The vertebral remains of the late Miocene great ape Hispanopithecus laietanus from Can Llobateres 2 (Vallès-Penedès Basin, NE Iberian Peninsula)\n",
      "New data on the ground beetles (Coleoptera: Carabidae) of Serbia\n",
      "The identity of the tropical African Polichne mukonja Griffini, 1908 (Orthoptera, Tettigoniidae, Phaneropterinae)\n",
      "Notes on the Galerucini from India and Sri Lanka, with description of Pyrrhalta warchalowskii sp. nov. from Tamil Nadu state, India (Coleoptera: Chrysomelidae: Galerucinae)\n",
      "Ts2631 Endolysin from the Extremophilic Thermus scotoductus Bacteriophage vB_Tsc2631 as an Antimicrobial Agent against Gram-Negative Multidrug-Resistant Bacteria\n",
      "\n",
      "\n",
      "Zn-O tetrahedral bond length variations in normal spinel oxides\n",
      "Analytical study of ancient pottery from the archaeological site of Aiani, northern Greece\n",
      "Η Παλαιολιθική Εποχή.The Palaeolithic period. Monograph.\n",
      "«Στρατηγικές επιβίωσης μετακινούμενων κυνηγών κτηνοτρόφων κατά τη διάρκεια της 4ης χιλιετίας. Στοιχεία από την εγκατάσταση στην ανατολική όχθη των Πηγών του Αγγίτη». 18ο Συνέδριο για το Αρχαιολογικό Εργο στη Μακεδονία και τη Θράκη (10-12/02/05), 45-80.\n",
      "Metric characteristics of ursid cheek teeth from Za Hájovnou Cave (Javoříčko Karst, the Czech Republic) and its taxonomical implication\n"
     ]
    }
   ],
   "source": [
    "classified_titles = pd.DataFrame({'text': pd.Series(titles), 'label': predictions})\n",
    "\n",
    "for title in classified_titles[classified_titles.label=='earth'].sample(5).text:\n",
    "    print(title)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for title in classified_titles[classified_titles.label=='life'].sample(5).text:\n",
    "    print(title)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "for title in classified_titles[classified_titles.label=='other'].sample(5).text:\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
